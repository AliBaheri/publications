% !TEX program = pdflatex
\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[nonatbib,final]{nips_2016}
%\usepackage{nips_2016}

\input{preamble/preamble}
\input{preamble/preamble_acronyms}
\input{preamble/preamble_math}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\title{The Generalized Reparameterization Gradient}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Francisco J.~R.~Ruiz\\
  University of Cambridge\\
  Columbia University\\
  \And
  Michalis K.~Titsias\\
  Athens University of\\Economics and Business\\
  \And
  David M.~Blei\\
  Columbia University\\
}

\begin{document}

\maketitle

% [todo] cite something after reparameterization gradient -dmb

\vspace*{-14pt}
\begin{abstract} \vspace*{-5pt}
  The reparameterization gradient has become a widely used method to obtain Monte Carlo gradients to optimize the variational objective. However, this technique does not easily apply to commonly used distributions such as beta or gamma without further approximations, and most practical applications of the reparameterization gradient fit Gaussian distributions. In this paper, we introduce \emph{the generalized reparameterization gradient}, a method that extends the reparameterization gradient to a wider class of variational distributions. Generalized reparameterizations use invertible transformations of the latent variables which lead to transformed distributions that weakly depend on the variational parameters. This results in new Monte Carlo gradients that combine reparameterization gradients and score function gradients. We demonstrate our approach on variational inference for two complex probabilistic models. The generalized reparameterization is effective: even a single sample from the variational distribution is enough to obtain a low-variance gradient.
\end{abstract}

  % The reparameterization gradient has become a widely used method for
  % obtaining Monte Carlo gradients to optimize the variational objective.
  % However, this technique only applies when fitting approximate
  % Gaussian distributions. In this paper, we introduce 
  % \emph{the generalized reparameterization gradient}, a method that
  % extends the reparameterization gradient to a wider class of
  % variational distributions. Generalized reparameterizations use
  % invertible transformations of the latent variables which lead to
  % transformed distributions that weakly depend on the variational
  % parameters.  This results in new Monte Carlo gradients that combine
  % reparameterization gradients and score function gradients.  We demonstrate
  % our approach on variational inference for two complex probabilistic
  % models, a deep exponential family and a nonconjugate factorization model.
  % The generalized reparameterization is effective: even a
  % single sample from the variational distribution is enough to obtain a
  % low-variance gradient.

% Remove \textfloatsep
\setlength{\textfloatsep}{7pt}

\input{sec01_introduction}
\input{sec02_background}
\input{sec03_generalizedrep}
\input{sec04_experiments}
\input{sec05_conclusions}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace*{-7pt}
\subsubsection*{Acknowledgments}
\vspace*{-5pt}
This project has received funding from the EU H2020 programme (Marie Sk\l{}odowska-Curie grant agreement 706760), NFS IIS-1247664, ONR N00014-11-1-0651, DARPA FA8750-14-2-0009, DARPA N66001-15-C-4032, Adobe, the John Templeton Foundation, and the Sloan Foundation. The authors would also like to thank Kriste Krstovski, Alp Kuckukelbir, and Christian A.\ Naesseth for helpful comments and discussions.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace*{-10pt}
\small
\bibliographystyle{apa}
\bibliography{bibReparamGrad}
%\bibliography{/Users/franrruiz87/Dropbox/Research/fjrrLibrary}


\end{document}
