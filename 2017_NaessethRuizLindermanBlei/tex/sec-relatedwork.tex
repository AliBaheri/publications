%!TEX root = main.tex
\section{Related Work}\label{sec:relatedwork}
The reparameterization trick has also been used in \gls{ADVI} \citep{Kucukelbir2015,Kucukelbir2016}. \gls{ADVI} applies a transformation to the random variables such that their support is on the reals and then places a Gaussian variational posterior approximation over the transformed variable~$\eps$. In this way, \gls{ADVI} allows for standard reparameterization, but it cannot fit gamma or Dirichlet variational posteriors, for example. Thus, \gls{ADVI} struggles to approximate probability densities with singularities, as noted by \citet{RuizTB2016}. In contrast, our approach allows us to apply the reparameterization trick on a wider class of variational distributions, which may be more appropriate when the exact posterior exhibits sparsity.

In the literature, we can find other lines of research that focus on extending the reparameterization gradient to other distributions. For the gamma distribution, \citet{Knowles2015} proposed a method based on approximations of the inverse cumulative density function; however, this approach is limited only to the gamma distribution and it involves expensive computations. 
For general expectations, \citet{Schulman2015} expressed the gradient as a sum of a reparameterization term and a correction term to automatically estimate the gradient % through a backpropagation algorithm
 in the context of stochastic computation graphs. However, it is not possible to directly apply it to variational inference with acceptance-rejection sampling. This is due to discontinuities in the accept--reject step and the fact that a rejection sampler produces a \emph{random number} of random variables. %However, they did not apply it to variational inference nor discuss any transforms. Furthermore, it is not possible to directly apply the theory to the rejection sampler.
Recently, another line of work has focused on applying reparameterization to discrete latent variable models \citep{Maddison2017,Jang2017} through a continuous relaxation of the discrete space.
%. The authors propose a continuous relaxation of the discrete space, accepting some bias, and estimate gradients using reparameterizations based on a Gumbel-softmax trick.

The \gls{G-REP} method \citep{RuizTB2016} exploits the decomposition of the gradient as $g_{\text{rep}}+g_{\text{cor}}$ by applying a transformation based on standardization of the sufficient statistics of $z$. Our approach differs from \gls{G-REP}: instead of searching for a transformation of $z$ that makes the distribution of $\eps$ weakly dependent on the variational parameters (namely, standardization), we do the opposite by choosing a transformation of a simple random variable $\eps$ such that the distribution of $z=h(\eps,\theta)$ is \emph{almost} equal to $q(z\g\theta)$. For that, we reuse the transformations typically used in rejection sampling. Rather than having to derive a new transformation for each variational distribution, we leverage decades of research on transformations in the rejection sampling literature \citep{devroye1986}. In rejection sampling, these transformations (and the distributions of $\eps$) are chosen so that they have high acceptance probability, which means we should expect to obtain $g_{\text{cor}}\approx 0$ with \gls{RS-VI}. In Sections~\ref{sec:examples} and \ref{sec:experiments} we compare \gls{RS-VI} with \gls{G-REP} and show that it exhibits significantly lower variance, thus leading to faster convergence of the inference algorithm.

Finally, another line of research in non-conjugate variational inference aims at developing more expressive variational families \citep{Salimans2015,Tran2016,Maaloe2016,Ranganath2016}. \gls{RS-VI} can extend the reparameterization trick to these methods as well, whenever rejection sampling is used to generate the random variables.

